<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="Diya Bhat" />
    <meta name="description" content="Describe your website">
    <link rel="shortcut icon" type="image/x-icon" href="/img/favicon.ico">
    <title>Project 1 (Exploratory Data Analysis)</title>
    <meta name="generator" content="Hugo 0.60.1" />
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="/css/main.css" />
    <link rel="stylesheet" type="text/css" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" />
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:200,400,200bold,400old" />
    
    <!--[if lt IE 9]>
			<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
			<script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
		<![endif]-->

    
  </head>

  <body>
    <div id="wrap">

      
      <nav class="navbar navbar-default">
  <div class="container">
    <div class="navbar-header">
      <a class="navbar-brand" href="/"><i class="fa fa-home"></i></a>
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
    <div class="navbar-collapse collapse" id="navbar">
      <ul class="nav navbar-nav navbar-right">
      
        
        <li><a href="/blog/">BLOG</a></li>
        
        <li><a href="/projects/">PROJECTS</a></li>
        
        <li><a href="/resume/">RESUME</a></li>
        
        <li><a href="/hobbies/">HOBBIES</a></li>
        
      
      </ul>
    </div>
  </div>
</nav>

      
      <div class="container">
        <div class="blog-post">
          <h3>
            <strong><a href="/project1/">Project 1 (Exploratory Data Analysis)</a></strong>
          </h3>
        </div>
        <div class="blog-title">
          <h4>
          January 1, 0001
            &nbsp;&nbsp;
            
          </h4>
        </div>
        <div class="panel panel-default">
          <div class="panel-body">
            <div class="blogpost">
              


<h1>
Exploring the Minimum Wage and Housing Market in the United States
</h1>
<p>Introduction: Two major predictors of the health of the US economy have always consistently been the housing market prices and the wage levels. I found two very interesting datasets, both on Kaggle. The first is a data set of US state and federal minimum wages from 1968 to 2017 per state and county, collected by the US Department of Labor. This dataset includes 2750 observations and 9 variables. The useful information it provides is: state, year, max and min wage, Consumer Price Index average, and wage equivalencies for 2018 so. The second dataset is a rent index data containing information about state-wise housing prices (per sqft) from 2010 to 2017, originally from Zillow. This dataset includes 13131 observations and 81 variables (because it has not been tidied). Most of the variables are months and years that have been recorded in a wide instead of long format. The other useful variables apart from these are: state, city, populaition rank, and metro. I chose these two datasets because I think that is has a very high potential for analysis and for making predictions about the future.</p>
<p>First I tided up the minimum wage dataset. This dataset seemed to have multiple rows that did not have any actual data and were just represented by three dots (…). I removed these because they were the equivalent of NAs. I also deleted the “Table_Data” column because it was repetitive information (high and low cols already existed for wages)</p>
<pre class="r"><code>#install.packages(&quot;dplyr&quot;)
#install.packages(&quot;tidyverse&quot;)
#install.packages(&quot;tidyr&quot;)
#install.packages(&quot;ggplot2&quot;)
library(dplyr)</code></pre>
<pre><code>## 
## Attaching package: &#39;dplyr&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     filter, lag</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     intersect, setdiff, setequal, union</code></pre>
<pre class="r"><code>library(tidyverse)</code></pre>
<pre><code>## -- Attaching packages ---------------------------------------------------- tidyverse 1.3.0 --</code></pre>
<pre><code>## &lt;U+2713&gt; ggplot2 3.2.1     &lt;U+2713&gt; purrr   0.3.3
## &lt;U+2713&gt; tibble  2.1.3     &lt;U+2713&gt; stringr 1.4.0
## &lt;U+2713&gt; tidyr   1.0.0     &lt;U+2713&gt; forcats 0.4.0
## &lt;U+2713&gt; readr   1.3.1</code></pre>
<pre><code>## -- Conflicts ------------------------------------------------------- tidyverse_conflicts() --
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()</code></pre>
<pre class="r"><code>library(tidyr)
library(ggplot2)

# read in both the datasets 
min_wage &lt;- read.csv(&quot;Minimum_Wage_Data.csv&quot;)
min_wage %&gt;% glimpse()</code></pre>
<pre><code>## Observations: 2,750
## Variables: 9
## $ Year        &lt;int&gt; 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 1968, 196…
## $ State       &lt;fct&gt; Alabama, Alaska, Arizona, Arkansas, California, Colorado,…
## $ Table_Data  &lt;fct&gt; ..., 2.1, 18.72 - 26.40/wk(b), 1.25/day(b), 1.65(b), 1.00…
## $ Footnote    &lt;fct&gt; , , (b), (b), (b), (b), , , , , , , , , , , , , , (b), , …
## $ High.Value  &lt;dbl&gt; 0.00000, 2.10000, 0.66000, 0.15625, 1.65000, 1.25000, 1.4…
## $ Low.Value   &lt;dbl&gt; 0.00000, 2.10000, 0.46800, 0.15625, 1.65000, 1.00000, 1.4…
## $ CPI.Average &lt;dbl&gt; 34.78333, 34.78333, 34.78333, 34.78333, 34.78333, 34.7833…
## $ High.2018   &lt;dbl&gt; 0.00, 15.12, 4.75, 1.12, 11.88, 9.00, 10.08, 9.00, 10.08,…
## $ Low.2018    &lt;dbl&gt; 0.00, 15.12, 3.37, 1.12, 11.88, 7.20, 10.08, 9.00, 9.00, …</code></pre>
<pre class="r"><code>housing &lt;- read.csv(&quot;Zillow_Housing_Data.csv&quot;)
housing %&gt;% glimpse()</code></pre>
<pre><code>## Observations: 13,131
## Variables: 81
## $ City.Code       &lt;int&gt; 6181, 12447, 17426, 39051, 13271, 40326, 18959, 6915,…
## $ City            &lt;fct&gt; New York, Los Angeles, Chicago, Houston, Philadelphia…
## $ Metro           &lt;fct&gt; New York, Los Angeles, Chicago, Houston, Philadelphia…
## $ County          &lt;fct&gt; Queens, Los Angeles, Cook, Harris, Philadelphia, Mari…
## $ State           &lt;fct&gt; NY, CA, IL, TX, PA, AZ, NV, TX, CA, TX, CA, FL, CA, I…
## $ Population.Rank &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16…
## $ Nov.10          &lt;int&gt; NA, 2184, 1563, 1198, 1092, 1087, 1188, 1057, 2070, 1…
## $ Dec.10          &lt;int&gt; NA, 2184, 1555, 1199, 1099, 1080, 1183, 1043, 2059, 1…
## $ Jan.11          &lt;int&gt; NA, 2183, 1547, 1199, 1094, 1071, 1178, 1037, 2043, 1…
## $ Feb.11          &lt;int&gt; NA, 2188, 1537, 1200, 1087, 1067, 1177, 1032, 2030, 1…
## $ Mar.11          &lt;int&gt; NA, 2189, 1526, 1203, 1080, 1065, 1178, 1028, 2020, 1…
## $ Apr.11          &lt;int&gt; NA, 2189, 1517, 1205, 1080, 1063, 1179, 1023, 2015, 1…
## $ May.11          &lt;int&gt; NA, 2188, 1507, 1204, 1083, 1059, 1176, 1018, 2014, 1…
## $ Jun.11          &lt;int&gt; NA, 2191, 1497, 1199, 1082, 1057, 1170, 1014, 2014, 1…
## $ Jul.11          &lt;int&gt; NA, 2189, 1493, 1194, 1082, 1056, 1163, 1011, 2014, 1…
## $ Aug.11          &lt;int&gt; NA, 2186, 1491, 1190, 1085, 1055, 1158, 1011, 2011, 1…
## $ Sep.11          &lt;int&gt; NA, 2183, 1489, 1188, 1095, 1050, 1154, 1014, 2011, 1…
## $ Oct.11          &lt;int&gt; NA, 2183, 1485, 1186, 1098, 1042, 1149, 1017, 2010, 1…
## $ Nov.11          &lt;int&gt; NA, 2182, 1480, 1188, 1094, 1033, 1144, 1018, 2007, 1…
## $ Dec.11          &lt;int&gt; 1746, 2178, 1483, 1190, 1085, 1029, 1140, 1026, 2013,…
## $ Jan.12          &lt;int&gt; 1752, 2172, 1484, 1194, 1080, 1029, 1137, 1033, 2019,…
## $ Feb.12          &lt;int&gt; 1764, 2175, 1485, 1196, 1083, 1029, 1133, 1038, 2020,…
## $ Mar.12          &lt;int&gt; 1778, 2177, 1489, 1199, 1087, 1032, 1130, 1043, 2015,…
## $ Apr.12          &lt;int&gt; 1792, 2183, 1494, 1200, 1091, 1034, 1127, 1045, 2006,…
## $ May.12          &lt;int&gt; 1804, 2186, 1496, 1199, 1092, 1038, 1127, 1050, 2000,…
## $ Jun.12          &lt;int&gt; 1813, 2190, 1493, 1197, 1094, 1041, 1127, 1053, 2002,…
## $ Jul.12          &lt;int&gt; 1814, 2192, 1491, 1193, 1096, 1043, 1128, 1057, 2009,…
## $ Aug.12          &lt;int&gt; 1810, 2189, 1491, 1193, 1095, 1048, 1126, 1060, 2020,…
## $ Sep.12          &lt;int&gt; 1805, 2189, 1498, 1197, 1092, 1053, 1125, 1059, 2032,…
## $ Oct.12          &lt;int&gt; 1806, 2185, 1509, 1201, 1090, 1059, 1124, 1058, 2038,…
## $ Nov.12          &lt;int&gt; 1817, 2183, 1513, 1203, 1089, 1066, 1125, 1055, 2034,…
## $ Dec.12          &lt;int&gt; 1831, 2186, 1517, 1199, 1087, 1073, 1126, 1059, 2031,…
## $ Jan.13          &lt;int&gt; 1851, 2194, 1514, 1201, 1083, 1078, 1128, 1062, 2029,…
## $ Feb.13          &lt;int&gt; 1870, 2203, 1511, 1208, 1081, 1079, 1130, 1067, 2034,…
## $ Mar.13          &lt;int&gt; 1888, 2212, 1512, 1218, 1083, 1079, 1131, 1068, 2041,…
## $ Apr.13          &lt;int&gt; 1901, 2222, 1527, 1227, 1089, 1080, 1131, 1070, 2053,…
## $ May.13          &lt;int&gt; 1918, 2229, 1544, 1236, 1093, 1083, 1132, 1070, 2064,…
## $ Jun.13          &lt;int&gt; 1941, 2236, 1560, 1248, 1095, 1087, 1134, 1073, 2076,…
## $ Jul.13          &lt;int&gt; 1968, 2239, 1562, 1258, 1094, 1087, 1138, 1078, 2085,…
## $ Aug.13          &lt;int&gt; 1987, 2246, 1568, 1265, 1093, 1088, 1141, 1084, 2095,…
## $ Sep.13          &lt;int&gt; 1999, 2255, 1574, 1269, 1087, 1091, 1145, 1091, 2102,…
## $ Oct.13          &lt;int&gt; 2004, 2267, 1584, 1277, 1083, 1096, 1148, 1099, 2111,…
## $ Nov.13          &lt;int&gt; 2014, 2278, 1585, 1287, 1082, 1099, 1152, 1105, 2118,…
## $ Dec.13          &lt;int&gt; 2026, 2283, 1593, 1295, 1085, 1104, 1155, 1114, 2132,…
## $ Jan.14          &lt;int&gt; 2040, 2285, 1606, 1297, 1092, 1107, 1157, 1119, 2140,…
## $ Feb.14          &lt;int&gt; 2052, 2283, 1616, 1296, 1098, 1107, 1158, 1123, 2144,…
## $ Mar.14          &lt;int&gt; 2058, 2285, 1619, 1293, 1105, 1103, 1157, 1125, 2148,…
## $ Apr.14          &lt;int&gt; 2064, 2283, 1614, 1294, 1108, 1099, 1157, 1129, 2153,…
## $ May.14          &lt;int&gt; 2071, 2285, 1612, 1296, 1108, 1096, 1155, 1134, 2155,…
## $ Jun.14          &lt;int&gt; 2080, 2288, 1615, 1301, 1106, 1095, 1155, 1137, 2155,…
## $ Jul.14          &lt;int&gt; 2104, 2303, 1628, 1310, 1111, 1099, 1155, 1141, 2159,…
## $ Aug.14          &lt;int&gt; 2132, 2320, 1636, 1322, 1121, 1103, 1158, 1146, 2169,…
## $ Sep.14          &lt;int&gt; 2169, 2343, 1649, 1334, 1136, 1112, 1162, 1153, 2184,…
## $ Oct.14          &lt;int&gt; 2191, 2367, 1658, 1344, 1150, 1122, 1168, 1159, 2201,…
## $ Nov.14          &lt;int&gt; 2206, 2395, 1672, 1355, 1164, 1132, 1172, 1161, 2217,…
## $ Dec.14          &lt;int&gt; 2214, 2423, 1677, 1367, 1175, 1140, 1176, 1164, 2237,…
## $ Jan.15          &lt;int&gt; 2216, 2445, 1677, 1377, 1182, 1146, 1175, 1168, 2256,…
## $ Feb.15          &lt;int&gt; 2229, 2464, 1668, 1384, 1185, 1152, 1175, 1176, 2274,…
## $ Mar.15          &lt;int&gt; 2241, 2479, 1668, 1389, 1183, 1159, 1174, 1185, 2291,…
## $ Apr.15          &lt;int&gt; 2248, 2493, 1669, 1394, 1178, 1164, 1176, 1195, 2304,…
## $ May.15          &lt;int&gt; 2253, 2502, 1667, 1402, 1176, 1171, 1180, 1205, 2317,…
## $ Jun.15          &lt;int&gt; 2251, 2511, 1668, 1410, 1179, 1176, 1186, 1215, 2328,…
## $ Jul.15          &lt;int&gt; 2246, 2521, 1670, 1419, 1179, 1185, 1192, 1224, 2337,…
## $ Aug.15          &lt;int&gt; 2259, 2536, 1668, 1425, 1177, 1191, 1197, 1230, 2342,…
## $ Sep.15          &lt;int&gt; 2276, 2546, 1660, 1428, 1175, 1194, 1197, 1233, 2343,…
## $ Oct.15          &lt;int&gt; 2304, 2555, 1652, 1428, 1179, 1193, 1197, 1234, 2343,…
## $ Nov.15          &lt;int&gt; 2322, 2564, 1649, 1429, 1184, 1191, 1196, 1231, 2340,…
## $ Dec.15          &lt;int&gt; 2334, 2577, 1653, 1431, 1189, 1193, 1198, 1229, 2345,…
## $ Jan.16          &lt;int&gt; 2335, 2596, 1668, 1436, 1196, 1198, 1204, 1230, 2360,…
## $ Feb.16          &lt;int&gt; 2331, 2607, 1671, 1439, 1200, 1206, 1210, 1235, 2379,…
## $ Mar.16          &lt;int&gt; 2329, 2622, 1682, 1442, 1205, 1218, 1217, 1241, 2400,…
## $ Apr.16          &lt;int&gt; 2334, 2637, 1684, 1444, 1206, 1228, 1222, 1244, 2414,…
## $ May.16          &lt;int&gt; 2339, 2662, 1686, 1446, 1211, 1236, 1225, 1245, 2428,…
## $ Jun.16          &lt;int&gt; 2345, 2687, 1687, 1446, 1218, 1240, 1227, 1245, 2438,…
## $ Jul.16          &lt;int&gt; 2344, 2704, 1685, 1443, 1222, 1240, 1227, 1241, 2442,…
## $ Aug.16          &lt;int&gt; 2336, 2716, 1681, 1440, 1223, 1238, 1227, 1236, 2441,…
## $ Sep.16          &lt;int&gt; 2324, 2723, 1675, 1438, 1220, 1238, 1228, 1234, 2442,…
## $ Oct.16          &lt;int&gt; 2318, 2731, 1668, 1437, 1216, 1239, 1230, 1235, 2449,…
## $ Nov.16          &lt;int&gt; 2321, 2740, 1656, 1437, 1211, 1241, 1234, 1239, 2457,…
## $ Dec.16          &lt;int&gt; 2321, 2748, 1644, 1435, 1209, 1244, 1237, 1245, 2465,…
## $ Jan.17          &lt;int&gt; 2322, 2753, 1632, 1430, 1212, 1247, 1239, 1250, 2469,…</code></pre>
<pre class="r"><code># start by tidying the min_wage dataset 
# remove all the rows missing wage data 
min_wage_1 &lt;- min_wage %&gt;% filter(!Table_Data == &quot;...&quot;)

# delete the &#39;Table_Data&#39; col because the high and low cols already exist 
min_wage_2 &lt;- min_wage_1 %&gt;% select(-Table_Data, -Footnote) </code></pre>
<p>Next, I tidied the second dataset (housing proce index). The data seemed to be unnecessarily wide here because every month and year had its own column. I used pivot_longer() to convert these columns to rows and make the data longer. I also needed to make the states from the two datasets match (acronym vs full length name), so I used recode to manually convert each state acronym to its full name (tried using the function abbr2state but it wasn’t working right). I did the same for year.</p>
<pre class="r"><code># now tidy up the housing dataset 
#install.packages(&quot;openintro&quot;)
library(openintro)</code></pre>
<pre><code>## Please visit openintro.org for free statistics materials</code></pre>
<pre><code>## 
## Attaching package: &#39;openintro&#39;</code></pre>
<pre><code>## The following object is masked _by_ &#39;.GlobalEnv&#39;:
## 
##     housing</code></pre>
<pre><code>## The following object is masked from &#39;package:ggplot2&#39;:
## 
##     diamonds</code></pre>
<pre><code>## The following objects are masked from &#39;package:datasets&#39;:
## 
##     cars, trees</code></pre>
<pre class="r"><code># make the wise data long using picot_longer() and then separate the merged month and year entries on the basis of a &quot;.&quot;. Finally, make the values from the previous wide data rows into a new column and name this &quot;Price.Per.Sqft&quot; 
housing_1 &lt;- housing %&gt;% pivot_longer(cols = c(&#39;Nov.10&#39;:&#39;Jan.17&#39;), names_to = &quot;Year&quot;, values_to = &quot;Price.Per.Sqft&quot;) %&gt;% separate(Year, into = c(&quot;Month&quot;, &quot;Year&quot;), convert = T, extra = &quot;merge&quot;, fill = &quot;right&quot;)
housing_1 %&gt;% glimpse()</code></pre>
<pre><code>## Observations: 984,825
## Variables: 9
## $ City.Code       &lt;int&gt; 6181, 6181, 6181, 6181, 6181, 6181, 6181, 6181, 6181,…
## $ City            &lt;fct&gt; New York, New York, New York, New York, New York, New…
## $ Metro           &lt;fct&gt; New York, New York, New York, New York, New York, New…
## $ County          &lt;fct&gt; Queens, Queens, Queens, Queens, Queens, Queens, Queen…
## $ State           &lt;fct&gt; NY, NY, NY, NY, NY, NY, NY, NY, NY, NY, NY, NY, NY, N…
## $ Population.Rank &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…
## $ Month           &lt;chr&gt; &quot;Nov&quot;, &quot;Dec&quot;, &quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;, &quot;Apr&quot;, &quot;May&quot;, &quot;Jun…
## $ Year            &lt;int&gt; 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 1…
## $ Price.Per.Sqft  &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1…</code></pre>
<pre class="r"><code># recode states to match the other dataset 
# I know this is tedious/inefficient and I apologize!
housing_1 &lt;- housing_1 %&gt;% mutate(State  =  recode(State, &quot;AK&quot; = &quot;Alaska&quot;, &quot;AL&quot; = &quot;Alabama&quot;, &quot;AR&quot; = &quot;Arkansas&quot;, &quot;AZ&quot; = &quot;Arizona&quot;, &quot;CA&quot; = &quot;California&quot;, &quot;CO&quot; = &quot;Colorado&quot;, &quot;CT&quot; = &quot;Connecticut&quot;, &quot;DC&quot; = &quot;District of Colombia&quot;, &quot;DE&quot; = &quot;Delaware&quot;, &quot;FL&quot; = &quot;Florida&quot;, &quot;GA&quot; = &quot;Georgia&quot;, &quot;HI&quot; = &quot;Hawaii&quot;, &quot;IA&quot; = &quot;Iowa&quot;, &quot;ID&quot; = &quot;Idaho&quot;, &quot;IL&quot; = &quot;Illinois&quot;, &quot;IN&quot; = &quot;Indiana&quot;, &quot;KS&quot; = &quot;Kansas&quot;, &quot;KY&quot; = &quot;Kentucky&quot;, &quot;LA&quot; = &quot;Louisiana&quot;, &quot;MA&quot; = &quot;Massachusetts&quot;, &quot;MD&quot; = &quot;Maryland&quot;, &quot;ME&quot; = &quot;Maine&quot;, &quot;MI&quot; = &quot;Mississippi&quot;, &quot;MN&quot; = &quot;Minnesota&quot;, &quot;MO&quot; = &quot;Montana&quot;, &quot;MS&quot; = &quot;Missouri&quot;, &quot;MT&quot; = &quot;Massachusetts&quot;, &quot;NC&quot; = &quot;North Carolina&quot;, &quot;ND&quot; = &quot;Nevada&quot;, &quot;NE&quot; = &quot;Nebraksa&quot;, &quot;NH&quot; = &quot;New Hampshire&quot;, &quot;NJ&quot; = &quot;New Jersey&quot;, &quot;NM&quot; = &quot;New Mexico&quot;, &quot;NV&quot; = &quot;Nevada&quot;, &quot;NY&quot; = &quot;New York&quot;, &quot;OH&quot; = &quot;Ohio&quot;, &quot;OK&quot; = &quot;Oklahoma&quot;, &quot;OR&quot; = &quot;Oregon&quot;, &quot;PA&quot; = &quot;Pennsylvania&quot;, &quot;RI&quot; = &quot;Rhode Island&quot;, &quot;SC&quot; = &quot;South Carolina&quot;, &quot;SD&quot; = &quot;South Dakota&quot;, &quot;TN&quot; = &quot;Tennessee&quot;, &quot;TX&quot; = &quot;Texas&quot;, &quot;UT&quot; = &quot;Utah&quot;, &quot;VA&quot; = &quot;Virginia&quot;, &quot;VT&quot; = &quot;Vermont&quot;, &quot;WA&quot; = &quot;Washington&quot;, &quot;WI&quot; = &quot;Wisconsin&quot;, &quot;WV&quot; = &quot;West Virginia&quot;, &quot;WY&quot; = &quot;Wyoming&quot;)) %&gt;% glimpse()</code></pre>
<pre><code>## Observations: 984,825
## Variables: 9
## $ City.Code       &lt;int&gt; 6181, 6181, 6181, 6181, 6181, 6181, 6181, 6181, 6181,…
## $ City            &lt;fct&gt; New York, New York, New York, New York, New York, New…
## $ Metro           &lt;fct&gt; New York, New York, New York, New York, New York, New…
## $ County          &lt;fct&gt; Queens, Queens, Queens, Queens, Queens, Queens, Queen…
## $ State           &lt;fct&gt; New York, New York, New York, New York, New York, New…
## $ Population.Rank &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…
## $ Month           &lt;chr&gt; &quot;Nov&quot;, &quot;Dec&quot;, &quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;, &quot;Apr&quot;, &quot;May&quot;, &quot;Jun…
## $ Year            &lt;int&gt; 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 1…
## $ Price.Per.Sqft  &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1…</code></pre>
<pre class="r"><code># recode the years as four digit numbers to match other dataset 
housing_2 &lt;- housing_1 %&gt;% mutate(Year = recode(Year, &quot;10&quot; = &quot;2010&quot;, &quot;11&quot; = &quot;2011&quot;, &quot;12&quot; = &quot;2012&quot;, &quot;13&quot; = &quot;2013&quot;, &quot;14&quot; = &quot;2014&quot;, &quot;15&quot; = &quot;2015&quot;, &quot;16&quot; = &quot;2016&quot;, &quot;17&quot; = &quot;2017&quot;)) %&gt;% glimpse()</code></pre>
<pre><code>## Observations: 984,825
## Variables: 9
## $ City.Code       &lt;int&gt; 6181, 6181, 6181, 6181, 6181, 6181, 6181, 6181, 6181,…
## $ City            &lt;fct&gt; New York, New York, New York, New York, New York, New…
## $ Metro           &lt;fct&gt; New York, New York, New York, New York, New York, New…
## $ County          &lt;fct&gt; Queens, Queens, Queens, Queens, Queens, Queens, Queen…
## $ State           &lt;fct&gt; New York, New York, New York, New York, New York, New…
## $ Population.Rank &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…
## $ Month           &lt;chr&gt; &quot;Nov&quot;, &quot;Dec&quot;, &quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;, &quot;Apr&quot;, &quot;May&quot;, &quot;Jun…
## $ Year            &lt;chr&gt; &quot;2010&quot;, &quot;2010&quot;, &quot;2011&quot;, &quot;2011&quot;, &quot;2011&quot;, &quot;2011&quot;, &quot;2011…
## $ Price.Per.Sqft  &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1…</code></pre>
<pre class="r"><code># delete the value column filled with NAs
housing_2 &lt;- housing_2 %&gt;% na.omit()</code></pre>
<p>After I tidied both datasets, I merged them by both state and year using inner_join(). I used inner_join() and not any other type of merge function because I did not want there to be any NAs after I merge them. Using inner_join() ensured that. I had to first use mutate() to coerce the ‘Year’ column from the min_wage dataset to have character and not numeric values so that it would match the housing dataset. When the inner join was done, 111,617 cases were dropped because the minimum wage dataset contains data for years from 1968-2017 while the housing dataset only contains years 2010-2017. I don’t see this being a problem because there are still over 850,000 observations so it is enough to conduct analysis on.</p>
<pre class="r"><code># convert Year values to categorical so that merging is possible  
min_wage_2 &lt;- min_wage_2 %&gt;% mutate(Year = as.factor(Year))
housing_2 &lt;- housing_2 %&gt;%  mutate(Year = as.factor(Year))

# join the two datasets by common variables &#39;Year&#39; and &#39;State&#39;
full_data &lt;- inner_join(housing_2, min_wage_2, by=c(&quot;Year&quot;,&quot;State&quot;))</code></pre>
<pre><code>## Warning: Column `Year` joining factors with different levels, coercing to
## character vector</code></pre>
<pre><code>## Warning: Column `State` joining factors with different levels, coercing to
## character vector</code></pre>
<pre class="r"><code>full_data %&gt;% glimpse()</code></pre>
<pre><code>## Observations: 855,937
## Variables: 14
## $ City.Code       &lt;int&gt; 6181, 6181, 6181, 6181, 6181, 6181, 6181, 6181, 6181,…
## $ City            &lt;fct&gt; New York, New York, New York, New York, New York, New…
## $ Metro           &lt;fct&gt; New York, New York, New York, New York, New York, New…
## $ County          &lt;fct&gt; Queens, Queens, Queens, Queens, Queens, Queens, Queen…
## $ State           &lt;chr&gt; &quot;New York&quot;, &quot;New York&quot;, &quot;New York&quot;, &quot;New York&quot;, &quot;New …
## $ Population.Rank &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…
## $ Month           &lt;chr&gt; &quot;Dec&quot;, &quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;, &quot;Apr&quot;, &quot;May&quot;, &quot;Jun&quot;, &quot;Jul…
## $ Year            &lt;chr&gt; &quot;2011&quot;, &quot;2012&quot;, &quot;2012&quot;, &quot;2012&quot;, &quot;2012&quot;, &quot;2012&quot;, &quot;2012…
## $ Price.Per.Sqft  &lt;int&gt; 1746, 1752, 1764, 1778, 1792, 1804, 1813, 1814, 1810,…
## $ High.Value      &lt;dbl&gt; 7.25, 7.25, 7.25, 7.25, 7.25, 7.25, 7.25, 7.25, 7.25,…
## $ Low.Value       &lt;dbl&gt; 7.25, 7.25, 7.25, 7.25, 7.25, 7.25, 7.25, 7.25, 7.25,…
## $ CPI.Average     &lt;dbl&gt; 224.9392, 229.5939, 229.5939, 229.5939, 229.5939, 229…
## $ High.2018       &lt;dbl&gt; 8.07, 7.91, 7.91, 7.91, 7.91, 7.91, 7.91, 7.91, 7.91,…
## $ Low.2018        &lt;dbl&gt; 8.07, 7.91, 7.91, 7.91, 7.91, 7.91, 7.91, 7.91, 7.91,…</code></pre>
<pre class="r"><code># remove all the NAs from dataset
full_data &lt;- full_data %&gt;% na.omit()

# give the cols more descriptive and intuitive names 
full_data &lt;- full_data %&gt;% rename(&quot;Max.Wage&quot; = &quot;High.Value&quot;, &quot;Min.Wage&quot; = &quot;Low.Value&quot;, &quot;Max.2018.Wage.Equivalent&quot; = &quot;High.2018&quot;,&quot;Min.2018.Wage.Equivalent&quot; = &quot;Low.2018&quot;) 
full_data %&gt;% glimpse()</code></pre>
<pre><code>## Observations: 855,937
## Variables: 14
## $ City.Code                &lt;int&gt; 6181, 6181, 6181, 6181, 6181, 6181, 6181, 61…
## $ City                     &lt;fct&gt; New York, New York, New York, New York, New …
## $ Metro                    &lt;fct&gt; New York, New York, New York, New York, New …
## $ County                   &lt;fct&gt; Queens, Queens, Queens, Queens, Queens, Quee…
## $ State                    &lt;chr&gt; &quot;New York&quot;, &quot;New York&quot;, &quot;New York&quot;, &quot;New Yor…
## $ Population.Rank          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…
## $ Month                    &lt;chr&gt; &quot;Dec&quot;, &quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;, &quot;Apr&quot;, &quot;May&quot;, &quot;J…
## $ Year                     &lt;chr&gt; &quot;2011&quot;, &quot;2012&quot;, &quot;2012&quot;, &quot;2012&quot;, &quot;2012&quot;, &quot;201…
## $ Price.Per.Sqft           &lt;int&gt; 1746, 1752, 1764, 1778, 1792, 1804, 1813, 18…
## $ Max.Wage                 &lt;dbl&gt; 7.25, 7.25, 7.25, 7.25, 7.25, 7.25, 7.25, 7.…
## $ Min.Wage                 &lt;dbl&gt; 7.25, 7.25, 7.25, 7.25, 7.25, 7.25, 7.25, 7.…
## $ CPI.Average              &lt;dbl&gt; 224.9392, 229.5939, 229.5939, 229.5939, 229.…
## $ Max.2018.Wage.Equivalent &lt;dbl&gt; 8.07, 7.91, 7.91, 7.91, 7.91, 7.91, 7.91, 7.…
## $ Min.2018.Wage.Equivalent &lt;dbl&gt; 8.07, 7.91, 7.91, 7.91, 7.91, 7.91, 7.91, 7.…</code></pre>
In this part of the project, I try to learn more about my dataset by:
</h4>
<p><em>1. Creating a new variable ‘Avg.Wage’, that is the mean of ‘Max.Wage’ and ‘Min.Wage’ (using mutate()). The reason why I created this variable is, I think it is a more representative value for wage than either maximum or minimum wage. I will be using this variable instead from now on whenever I want to find relationships between wage levels and anything else.</em></p>
<p><em>2. Creating a correlation matrix with all of my numeric variables using select() and cor(). I think this is a crucial part of finding out trends in my dataset. The numeric variables I chose to include are ‘Population.Rank’, ‘CPI.Average’, and ‘Avg.Wage’. I deliberately left out variables like ‘Max.2018.Wage.Equivalent’ because it would have had a misleading perfect correlation with ‘Avg.Wage’. In other words, some of these extra variables are redundant because they just aim to portray the same information in a more meaningful and digestible way.</em></p>
<p><em>3. Creating summary statistics for all the numeric variables, using summary(), to find out more about them briefly before I delve further</em></p>
<p><em>4. Creating more detailed and specialized summary statistics using summarize() for wage data after grouping based on the categorical variable ‘State’ using group_by(). The summary statistics include mean, sd, se, max, min, count, and range. I wanted to use these statistics to figure out which states had the highest mean wages. I made this task simpler using arrange() in descending order. The state with the highest mean wage turned out to be New York!</em></p>
<p><em>5. Creating more summary statistics, this time for house pricing data, again using summarize(), this time grouped by the categorical variable ‘Year’ using group_by().</em></p>
<p><em>6. Creating summary statistics like in the previous case and then using filter() to find the state with the highest housing price per sqft. It turned out to be California at $2106.476!</em></p>
<pre class="r"><code>full_data &lt;- full_data %&gt;% mutate(Year = as.numeric(Year))

#1. create a new variable that is the average of Max and Min wage using mutate(). We will use this from now on instead of high or low wage because it is a better representation of wage levels
full_data &lt;- full_data %&gt;% mutate(Avg.Wage = (Max.Wage + Min.Wage) / 2)
full_data %&gt;% glimpse()</code></pre>
<pre><code>## Observations: 855,937
## Variables: 15
## $ City.Code                &lt;int&gt; 6181, 6181, 6181, 6181, 6181, 6181, 6181, 61…
## $ City                     &lt;fct&gt; New York, New York, New York, New York, New …
## $ Metro                    &lt;fct&gt; New York, New York, New York, New York, New …
## $ County                   &lt;fct&gt; Queens, Queens, Queens, Queens, Queens, Quee…
## $ State                    &lt;chr&gt; &quot;New York&quot;, &quot;New York&quot;, &quot;New York&quot;, &quot;New Yor…
## $ Population.Rank          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…
## $ Month                    &lt;chr&gt; &quot;Dec&quot;, &quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;, &quot;Apr&quot;, &quot;May&quot;, &quot;J…
## $ Year                     &lt;dbl&gt; 2011, 2012, 2012, 2012, 2012, 2012, 2012, 20…
## $ Price.Per.Sqft           &lt;int&gt; 1746, 1752, 1764, 1778, 1792, 1804, 1813, 18…
## $ Max.Wage                 &lt;dbl&gt; 7.25, 7.25, 7.25, 7.25, 7.25, 7.25, 7.25, 7.…
## $ Min.Wage                 &lt;dbl&gt; 7.25, 7.25, 7.25, 7.25, 7.25, 7.25, 7.25, 7.…
## $ CPI.Average              &lt;dbl&gt; 224.9392, 229.5939, 229.5939, 229.5939, 229.…
## $ Max.2018.Wage.Equivalent &lt;dbl&gt; 8.07, 7.91, 7.91, 7.91, 7.91, 7.91, 7.91, 7.…
## $ Min.2018.Wage.Equivalent &lt;dbl&gt; 8.07, 7.91, 7.91, 7.91, 7.91, 7.91, 7.91, 7.…
## $ Avg.Wage                 &lt;dbl&gt; 7.25, 7.25, 7.25, 7.25, 7.25, 7.25, 7.25, 7.…</code></pre>
<pre class="r"><code>#2. correlation matrix for numeric variables. This will be used later ot make a correlation heatmap  
cor_matrix &lt;- full_data %&gt;% select_if(is.numeric) %&gt;% select(-City.Code, -Max.Wage, -Min.Wage, -Max.2018.Wage.Equivalent, -Min.2018.Wage.Equivalent, -Year) %&gt;% scale %&gt;% cov %&gt;% round(2)
cor_matrix</code></pre>
<pre><code>##                 Population.Rank Price.Per.Sqft CPI.Average Avg.Wage
## Population.Rank            1.00          -0.14        0.01    -0.11
## Price.Per.Sqft            -0.14           1.00        0.07     0.26
## CPI.Average                0.01           0.07        1.00     0.26
## Avg.Wage                  -0.11           0.26        0.26     1.00</code></pre>
<pre class="r"><code>#3. snapshot of summary statistics for the main 4 numeric variables 
full_data %&gt;% select_if(is.numeric) %&gt;% select(-Max.Wage, -Min.Wage, -City.Code, -Max.2018.Wage.Equivalent, -Min.2018.Wage.Equivalent) %&gt;% summary()</code></pre>
<pre><code>##  Population.Rank      Year      Price.Per.Sqft   CPI.Average   
##  Min.   :    1   Min.   :2010   Min.   :  470   Min.   :218.1  
##  1st Qu.: 3229   1st Qu.:2012   1st Qu.: 1014   1st Qu.:229.6  
##  Median : 6501   Median :2014   Median : 1245   Median :236.7  
##  Mean   : 6515   Mean   :2014   Mean   : 1416   Mean   :233.5  
##  3rd Qu.: 9768   3rd Qu.:2015   3rd Qu.: 1568   3rd Qu.:237.0  
##  Max.   :13131   Max.   :2017   Max.   :22744   Max.   :245.1  
##     Avg.Wage     
##  Min.   : 4.625  
##  1st Qu.: 7.250  
##  Median : 7.250  
##  Mean   : 7.572  
##  3rd Qu.: 8.050  
##  Max.   :11.000</code></pre>
<pre class="r"><code>#4. summary statistics for each state&#39;s average wage level over 7 years, including mean, sd, se, max, min, count, and range. Finally, this data is arranged in descending order according to mean_wage to more easily see which states had the highest mean wage level and which the lowest 
full_data %&gt;% group_by(State) %&gt;% summarize(mean_wage = mean(Avg.Wage, na.rm=T), sd_wage = sd(Avg.Wage, na.rm = T), n = n(), se_wage = sd_wage/sqrt(n), max_wage = max(Avg.Wage), min_wage = min(Avg.Wage), range = max_wage - min_wage) %&gt;% arrange(desc(mean_wage)) %&gt;% glimpse()</code></pre>
<pre><code>## Observations: 42
## Variables: 8
## $ State     &lt;chr&gt; &quot;Washington&quot;, &quot;Oregon&quot;, &quot;Vermont&quot;, &quot;Connecticut&quot;, &quot;Californ…
## $ mean_wage &lt;dbl&gt; 9.203558, 9.062127, 8.816620, 8.706667, 8.667888, 8.524942,…
## $ sd_wage   &lt;dbl&gt; 0.35819044, 0.40026598, 0.48533156, 0.53917626, 0.75440408,…
## $ n         &lt;int&gt; 22272, 13552, 6887, 13125, 55698, 25780, 852, 41512, 2475, …
## $ se_wage   &lt;dbl&gt; 0.0024001279, 0.0034383254, 0.0058482171, 0.0047063162, 0.0…
## $ max_wage  &lt;dbl&gt; 11.00, 9.75, 10.00, 10.10, 10.00, 11.00, 9.80, 8.25, 9.60, …
## $ min_wage  &lt;dbl&gt; 8.55, 8.40, 8.15, 8.25, 8.00, 8.00, 7.75, 8.00, 7.40, 7.25,…
## $ range     &lt;dbl&gt; 2.45, 1.35, 1.85, 1.85, 2.00, 3.00, 2.05, 0.25, 2.20, 2.45,…</code></pre>
<pre class="r"><code>#5. summary statistics for each year&#39;s Consumer Price Index, including mean, count, max, and min
full_data %&gt;% group_by(State) %&gt;% summarize(mean_price = mean(Price.Per.Sqft, na.rm=T), n = n(), max_price = max(Price.Per.Sqft), min_price = min(Price.Per.Sqft), sd_price = sd(Price.Per.Sqft, na.rm = T), se_price = sd_price/sqrt(n)) %&gt;% glimpse()</code></pre>
<pre><code>## Observations: 42
## Variables: 7
## $ State      &lt;chr&gt; &quot;Alaska&quot;, &quot;Arizona&quot;, &quot;Arkansas&quot;, &quot;California&quot;, &quot;Colorado&quot;,…
## $ mean_price &lt;dbl&gt; 1565.5258, 1199.4697, 909.9507, 2106.4757, 1540.4579, 1767…
## $ n          &lt;int&gt; 852, 8925, 10923, 55698, 12993, 13125, 2850, 40340, 26462,…
## $ max_price  &lt;int&gt; 1983, 8389, 1686, 17786, 5098, 6645, 2391, 22744, 11490, 3…
## $ min_price  &lt;int&gt; 1012, 621, 581, 706, 759, 1058, 970, 630, 582, 945, 722, 6…
## $ sd_price   &lt;dbl&gt; 195.9502, 719.5553, 165.9041, 1399.1963, 566.4995, 634.067…
## $ se_price   &lt;dbl&gt; 6.713145, 7.616581, 1.587399, 5.928691, 4.969868, 5.534592…</code></pre>
<pre class="r"><code>#6. using summary statistics to find which state has the highest mean housing price per sqft
full_data %&gt;% group_by(State) %&gt;% summarize(mean_price = mean(Price.Per.Sqft, na.rm=T)) %&gt;% filter(mean_price == max(mean_price))</code></pre>
<pre><code>## # A tibble: 1 x 2
##   State      mean_price
##   &lt;chr&gt;           &lt;dbl&gt;
## 1 California      2106.</code></pre>
<p>The scatter plot below shows the relationship between year and average wage depending on state. This is an interesting graph because identifying the states that experienced steep increases in average wage levels could give us some insight into relative state of economies. For example, the states for which the minimum wage stayed exactly the same over a period of 7 years include: Wyoming, Wisconsin, Texas, and Utah. On the other hand, the states for which the mimumum wage saw a great leap between years included: Massachusetts, Minnesota, Montana, New York, and Washington. This is extremely interesting because perhaps a variable that can further be added to this data set is the categorical variable that either groups a state as republican or democratic. Although, of course, the states I listed above are not representative, it seems as though primarily blue states underwent major minumum wage transitions while red states remained fairly static.</p>
<pre class="r"><code># change &#39;Year&#39; to a numeric variable just for this scatterplot 
full_data &lt;- full_data %&gt;% mutate(Year = as.numeric(Year))

# scatterplot showing the relationship between years and average wage level, color-coded based on state
ggplot(full_data, aes(x = Year, y = Avg.Wage, color = State)) + ggtitle(&quot;Relationship between Time and Average Wage Depending on State&quot;) + xlab(&quot;Years&quot;) + ylab(&quot;Avg Wage ($/hr)&quot;) + theme(legend.position = &quot;right&quot;) + geom_point(size = 4) +  geom_line() +
  guides(shape = guide_legend(override.aes = list(size = 0.5))) +
  guides(color = guide_legend(override.aes = list(size = 0.5))) +
  theme(legend.title = element_text(size = 7), legend.text = element_text(size = 7))</code></pre>
<p><img src="/Project1_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>The scatter plot below shows the relationship between housing pricing per sqft and average wage faceted based on state. From the plots below, it seems like a few states like California, Massachusetts, and Hawaii experienced ggradual increases in housing price index over the years. However, most of the states seem to have remained fairly stable with respect to this variable. One uniting factor is that all these states seem to be on the rise for housing prices. One interesting anomaly to this statement, however, is Rhode Island, which seemed to experience a little dip in 2012.</p>
<pre class="r"><code># faceted (by State) bar plot demonstrating relationship between year and house pricing per sqft
ggplot(full_data, aes(x = Year, y = Price.Per.Sqft, fill = State)) + geom_bar(stat = &quot;summary&quot;, fun.y = &quot;mean&quot;, width = 0.7) + scale_y_continuous(name = &quot;Housing Price ($/Sqft)&quot;) +  stat_summary(fun.data = mean_se, geom = &quot;errorbar&quot;, width=0.3) + ggtitle(&quot;Relationship between Year and House Pricing per sqft based on State&quot;) + facet_wrap(~State) + theme(axis.text.x = element_text(angle = 45, size = 8, hjust = 1))  + theme(legend.position = &quot;none&quot;)</code></pre>
<p><img src="/Project1_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>The bar plot below shows the relationship between year and mean Consumer Price Index for each state. There seems to be a general gradual increase in CPI from 2010 to 2016 for all of the states. This can be explained by the fact that industries in the United States have generally flourished since 2010 and inflation has led to a steady increase in pricing of goods and services.</p>
<pre class="r"><code># faceted (by State) bar plot demonstrating relationship between year and CPI
ggplot(full_data, aes(x = Year, y = CPI.Average, fill = Year)) + geom_bar(stat = &quot;summary&quot;, fun.y = &quot;mean&quot;, width = 0.7) + scale_y_continuous(name = &quot;CPI.Average&quot;) +  stat_summary(fun.data = mean_se, geom=&quot;errorbar&quot;, width=0.3) + ggtitle(&quot;Relationship between Year and Consumer Price Index based on State&quot;) + facet_wrap(~State) + theme(axis.text.x = element_text(angle = 45, size = 8, hjust = 1))</code></pre>
<p><img src="/Project1_files/figure-html/unnamed-chunk-7-1.png" width="672" />
After creating a correlation heatmap for all the main numeric variables, I found some interesting relationships. Two sets of variables that appeared to have slightly significant positive correlations were: average wage and consumer price index, and average wage and housing price per sqft. The former case makes sense because as inflation made goods and services more expensive, governments had to raise minimum wage levels so that the general public could maintain their quality of life and standard of living. The latter case also makes sense because as housing prices increase, employers are pressured to raise the wages they are paying to the laborforce so that they can afford rent. Average wage and population ranking had a negative correlation. This also makes sense because a state highly ranked in terms of population (one that has a very high population) will require a high demand for land, making housing prices go up. California and New York, for example, are two of the most populated states and both of them have very high relative minimum wages. Consumer price index and population rank are not correlated at all, which is surprising.</p>
<pre class="r"><code># correlation heatmap with all the numeric variables for the dataset 
#install.packages(&quot;reshape2&quot;)
library(reshape2)</code></pre>
<pre><code>## 
## Attaching package: &#39;reshape2&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:openintro&#39;:
## 
##     tips</code></pre>
<pre><code>## The following object is masked from &#39;package:tidyr&#39;:
## 
##     smiths</code></pre>
<pre class="r"><code>cor_mat &lt;- melt(cor_matrix)
ggplot(cor_mat, aes(x=Var1, y=Var2, fill = value)) + geom_tile() + theme(axis.text.x = element_text(angle = 45, size = 8, hjust = 1)) + scale_fill_gradient(low = &quot;mistyrose2&quot;, high = &quot;mediumvioletred&quot;) + labs(fill = &quot;Correlation&quot;) + ggtitle(&quot;Correlation between Numerical Variables in Dataset&quot;)</code></pre>
<p><img src="/Project1_files/figure-html/unnamed-chunk-8-1.png" width="672" />
Lastly, I conducted a Principal Component Analysis (PCA). I followed the steps that we discussed in class. My data was already prepped from previous actions when I was tidying it up. I just had to use scale() so that the data would be normalized. I then ran princomp() on the scaled data. After finding the eigenvalues by squaring the standard deviations and rounding these, I found that there were three principal components with variance proportions of 0.43, 0.33, and 0.24 respectively. I tried to use the rules of thumb we discussed in class as much as possible:
<em>1. The scree plot looked relatively flat, with no flagrant ‘elbows’ with the first three PCs.</em>
<em>2. The cumulative proportion of the first three PCs is more than 80% (83.5%).</em><br />
<em>3. Following Kaiser’s rule, only PC1 and PC2 had eigenvalues higher than 1 (1.454 and 1.044 respectively).</em>
Based on this analysis, I decided to use the first 2 PCs.</p>
<p>PC1 seems to account for most of the variation, seeing as its prop of variation is 0.36.
From the loadings plot, we can see that the two most higly correlated variables are: Average wage with CPI Average and Average wage with Price per sqft. This is evident because for the arrows for these two sets of variables are closest together, symbolizing a high degree of correlation. This matches up to the analysis from the correlation heatmap. The two most distant variables (ones with the lowest correlation with each other) are Price per sqft and Population rank. Population rank seems to have a generally lower correlation with all the other variables.</p>
<pre class="r"><code># select only the relevant numeric variables 
full_data_1 &lt;- full_data %&gt;% select(-City.Code, -Max.Wage, -Min.Wage, -Max.2018.Wage.Equivalent, -Min.2018.Wage.Equivalent, -Year)

# scale numeric variables  
full_data_numeric &lt;- full_data_1 %&gt;% select_if(is.numeric) %&gt;% scale

# use the princomp() function on these scaled variables 
full_data_pca &lt;- princomp(full_data_numeric)

# view pca information including: sdev, varprop, and cum prop of components AND the loadings information
summary(full_data_pca, loadings=T)</code></pre>
<pre><code>## Importance of components:
##                           Comp.1    Comp.2    Comp.3    Comp.4
## Standard deviation     1.2059472 1.0215881 0.9168034 0.8133363
## Proportion of Variance 0.3635776 0.2609109 0.2101323 0.1653792
## Cumulative Proportion  0.3635776 0.6244885 0.8346208 1.0000000
## 
## Loadings:
##                 Comp.1 Comp.2 Comp.3 Comp.4
## Population.Rank  0.324  0.687  0.646       
## Price.Per.Sqft  -0.534 -0.313  0.649 -0.443
## CPI.Average     -0.445  0.634 -0.397 -0.493
## Avg.Wage        -0.642  0.168         0.745</code></pre>
<pre class="r"><code># convery standard devs to eigenvalues by squaring them
eigval &lt;- full_data_pca$sdev^2 
eigval</code></pre>
<pre><code>##    Comp.1    Comp.2    Comp.3    Comp.4 
## 1.4543087 1.0436423 0.8405284 0.6615159</code></pre>
<pre class="r"><code># proportion of var explained by each PC; cumulative varprops
varprop &lt;- round(eigval/sum(eigval),2) 
varprop </code></pre>
<pre><code>## Comp.1 Comp.2 Comp.3 Comp.4 
##   0.36   0.26   0.21   0.17</code></pre>
<pre class="r"><code># non-cumulative varprops 
round(cumsum(eigval)/sum(eigval),2)</code></pre>
<pre><code>## Comp.1 Comp.2 Comp.3 Comp.4 
##   0.36   0.62   0.83   1.00</code></pre>
<pre class="r"><code># plot showing the proportion of variance explained by each principal component 
ggplot() + geom_bar(aes(y=varprop,x=1:4),stat=&quot;identity&quot;)+xlab(&quot;&quot;)+geom_path(aes(y=varprop,x=1:4))+
 geom_text(aes(x=1:4,y = varprop, label = round(varprop, 2)),vjust=1,col=&quot;white&quot;,size=5)+
 ggtitle(&quot;Proportion of Variance Explained by each PC&quot;)</code></pre>
<p><img src="/Project1_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<pre class="r"><code># plot PC1 and PC2 
full_data_df &lt;- data.frame(PC1 = full_data_pca $ scores[, 1], PC2 = full_data_pca $ scores[, 2])
ggplot(full_data_df, aes(PC1, PC2)) + geom_point() + ggtitle(&quot;PC1 and PC2&quot;)</code></pre>
<p><img src="/Project1_files/figure-html/unnamed-chunk-9-2.png" width="672" /></p>
<pre class="r"><code># loadings plot
full_data_pca $ loadings[1:4,1:2] %&gt;% as.data.frame %&gt;% rownames_to_column %&gt;%
ggplot() + geom_hline(aes(yintercept = 0), lty = 2) +
 geom_vline(aes(xintercept = 0), lty = 2) + ylab(&quot;PC2&quot;) + xlab(&quot;PC1&quot;) +
 geom_segment(aes(x = 0, y = 0, xend = Comp.1, yend = Comp.2), arrow = arrow(), col = &quot;red&quot;) +
 geom_label(aes(x = Comp.1*1.1, y = Comp.2*1.1,label=rowname)) + ggtitle(&quot;Loadings Plot&quot;)</code></pre>
<p><img src="/Project1_files/figure-html/unnamed-chunk-9-3.png" width="672" /></p>
<pre class="r"><code># biplot
full_data %&gt;% mutate(PC1 = full_data_pca$scores[,1], PC2 = full_data_pca$scores[, 2]) %&gt;%
 ggplot(aes(x = PC1, y = PC2, color = State)) + geom_point() + ggtitle(&quot;Correlation Biplot&quot;)</code></pre>
<p><img src="/Project1_files/figure-html/unnamed-chunk-9-4.png" width="672" /></p>

              <hr>
              <div class="related-posts">
                <h5>Related Posts</h5>
                
              </div>
            </div>
          </div>
          <hr>
        <div class="disqus">
  <div id="disqus_thread"></div>
  <script type="text/javascript">

    (function() {
      
      
      if (window.location.hostname == "localhost")
        return;

      var disqus_shortname = '';
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  <a href="http://disqus.com/" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>
        </div>
      </div>
      
    </div>

    
    <footer>
  <div id="footer">
    <div class="container">
      <p class="text-muted">&copy; All rights reserved. Powered by <a href="https://gohugo.io/">Hugo</a> and
      <a href="http://www.github.com/nurlansu/hugo-sustain/">sustain</a> with ♥</p>
    </div>
  </div>
</footer>
<div class="footer"></div>


<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>

<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
<script src="/js/docs.min.js"></script>
<script src="/js/main.js"></script>

<script src="/js/ie10-viewport-bug-workaround.js"></script>


    
  </body>
</html>
