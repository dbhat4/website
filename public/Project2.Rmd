---
title: "Project 2"
author: "Diya Bhat"
date: "11/21/2019"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
<h1> Modeling the Important Factors that Contribute to Historical Mass Conflicts </h1>

0. Introduction: Throughout history, we have seen wars and conflicts of varying scale. From the Civil War 1865 to WWI starting in 1914, we have seen schisms root from a multitude of reasons. Our inability to determine which exact factors predict the coming of a war, puts us in an eternal state of peril because we do not know how to avoid what we cannot identify as a direct causation. This dataset aims to aid this school of thought by considering a wide range of variables that potentially ignited a war or conflict. These variables are mostly binary. The conflict response is measured via the numeric variables: deaths (extent of damage) and months (duration). Most of the explanatory variables are binary apart from numGroups, which is numeric and deatails the number of participants involved in the conflict. The variables that I will be including in my analysis are: nat.grp (nation vs group in other nation), grp.grpSame (groups in same nation), grp.grpDiff (groups in different nations), difWealth (whether or not there were differences in financial resources between groups), difRelig (whether or not there were differences in religion between groups), sameLanguage (whether or not there were differences in language between groups), difLegal (Whether or not there were differences in legal systems and rules between groups), enjoyFight (whether or not the participants enjoyed fighting), overpopulated (whether or not origin countries were overpopulated), and fightForPay (whether the participants were mecenaries and had financial motivations).



*I first tidied up the dataset by only selecting the relevant variables, using pivot_longer() to combine the three variables 'international', 'colonial', and 'revolution' into a single variable called 'Scale_of_Conflict'.*
```{R}
r = getOption("repos")
r["CRAN"] = "http://cran.us.r-project.org"
options(repos = r)
#install.packages("dplyr")
#install.packages("tidyverse")
#install.packages("tidyr")
#install.packages("ggplot2")
library(dplyr)
library(tidyr)
library(tidyverse)


# load the package and library containing the dataset "Quarrel"
#install.packages("HistData")
library(HistData)
data(Quarrels)

# only select the relevant variables 
Quar_tidy <- Quarrels %>% select(ID, year, international, colonial, revolution, nat.grp, grp.grpSame, grp.grpDif, numGroups, months, deaths, difWealth, difRelig, sameLanguage, difLegal, enjoyFight, overpopulated, fightForPay)

# combine the vars 'international', 'colonial', and 'revolution' into a single variable called 'scale of conflict. This will be the categorical variable for future analysis' 
Quar_tidier <- Quar_tidy %>% pivot_longer(cols = c('international', 'colonial', 'revolution'), names_to = "Scale_of_Conflict", values_to = "Yes") %>% filter(Yes == "1") %>% select(-Yes)

```
<h4>1. I conducted a MANOVA to test whether either of my numeric variables 'deaths' or 'months' show a mean difference across levels of the categorical variable 'Scale_of_Conflict'. Before this, I eyeballed it and tested to see if the data passed some of the assumptions. The data failed the multivariate normality assumptions because when deaths was plotted against months, there seemed to be some sort of pre-existing relationship between the two variables. The data also fails the homogeneity assumption because there doesn't seem to be equality of covariances between the two variables. However, the MANOVA was conducted anyway.</h4>

```{R}
# testing roughly for the multivariate assumption
ggplot(Quar_tidier, aes(x = deaths, y = months)) +
geom_point(alpha = .5) + ggtitle("Correlation between Deaths and Months of Conflict")

# testing roughly for the homogeneity assumption
covmats<- Quar_tidier %>% select(Scale_of_Conflict, deaths, months) %>% group_by(Scale_of_Conflict) %>% do(covs = cov(.[2:3]))
for(i in 1:3){print(covmats$covs[i])}

# conduct the MANOVA test 
man <- manova(cbind(deaths, months) ~ Scale_of_Conflict, data = Quar_tidier)
summary(man)
summary.aov(man) # univariate ANOVAs 

# pairwise t-tests for either numeric variable 
pairwise.t.test(Quar_tidier $ deaths, Quar_tidier$Scale_of_Conflict, p.adj="none")
pairwise.t.test(Quar_tidier $ months, Quar_tidier$Scale_of_Conflict, p.adj="none")

```
*MANOVA Report: A (MANOVA) was conducted to determine the effect of scale of conflict (international, colonial, revolution) on two dependent variables (number of deaths and months of conflict). Significant differences were found among the three scales of conflict on the two dependent measures, Univariate analyses of variance (ANOVAs) for each dependent variable were conducted as follow-up tests to the MANOVA, using the Bonferroni method for controlling Type I error rates for multiple comparisons. The univariate ANOVAs for deaths and months were also found to be significant (p-values < 2.2e-16 and 0.03139). Post hoc analysis was performed conducting pairwise comparisons to determine which scale of conflict differed in deaths and months. All three conflict levels were found to differ significantly from each other in terms of deaths and months after adjusting for multiple comparisons (bonferroni). For the deaths reponse variable, significant differences were found between international and colonial (p-value < 2e-16) and revolution and international (p-value 1.9e-13). For the response variable months, significant differences were found between revolution and colonial only (p-value 0.009).*
*In total I performed 1 MANOVA, 2 ANOVAs, and 6 t tests for a total of 9 tests. Therefore, the probability of at least one type I error (if unadjusted) would have been 1-(1-0.05)^9 = 0.370. After the Bonferroni correction, the new alpha level was 0.05/9 = 0.00556 (which is what was used to determine significance for all the tests).*



<h4>2. I chose to conduct a randomization test where I compared the mean deaths between 2 of the 3 levels of my categorical variable 'Scale of Conflict'. To do this, I created a new dataset called 'Quar_rand' by filtering out all the 'international' rows and only chose 'colonial' and 'revolution' rows because these two categories have more similar means.</h4>
*Null hytpothesis: The mean number of deaths between colonial and revolution conflicts are the same.*
*Alternative hypothesis: The mean number of deaths between colonial and revolution conflicts are not the same.*
```{r}
# create a new dataset where all the 'international' rows are filtered out so that only 'revolution' and 'colonial' mean deaths are compared for the test 
Quar_rand <- Quar_tidier %>% filter(Scale_of_Conflict != "international")

# find the mean difference in deaths between the groups 'colonial' and 'revolution'
Quar_rand %>% group_by(Scale_of_Conflict) %>% summarize(mean = mean(deaths)) %>% summarize(diff(mean))

# sample from these two groups many times and create a vector that has all the mean differences from each of these samples 
rand_dist <- vector()
for(i in 1:5000){
new <- data.frame(deaths = sample(Quar_rand$deaths),Scale_of_Conflict = Quar_rand$Scale_of_Conflict)
rand_dist[i] <- mean(new[new$Scale_of_Conflict == "revolution",]$deaths)-
mean(new[new$Scale_of_Conflict == "colonial",]$deaths)}
{hist(rand_dist); abline(v = 729932.9	,col="red")}

# find the p-value using this mean and the vector 
mean(rand_dist > 729932.9)*2 

``` 
*The calculated mean difference in deaths between the 'colonial' and 'revolution' levels of the categorical variable 'Scale of Conflict' was found to be 729932.9 (can be seen plotted as the red line on the graph above). The p-value using this value is 8e-04. Since 8e-04 is much lower than 0.05, we reject the null hypothesis and conclude that the mean deaths between colonial and revolution conflicts are not the same. The histogram above shows the distribution of the mean differences in the rand_dist vector, plotted in the context of the actual mean.*



<h4>3. Next, I built a linear regression model predicting the numeric response variable 'deaths' from the numeric variables 'months' and 'numGroups', including their interaction.</h4>
<h4>I checked ssumptions of linearity, normality, and homoskedasticity. To see if the homoskedasticity assumption is met, I ran the Breusch Pagan test and got a p-value of 1.633e-08, which is lower than 0.05 and leads us to conclude that this assumption is not met. The graph of residuals demonstrates that linearity assumption is not met because the data points show a clear pattern. To test the normality assumption, I plotted a histogram of the residuals and ran the One-sample Kolmogorov-Smirnov test. The right-skewed plot and the low p-value of < 2.2e-16 from the KS test further confirm that this normality assumption is not met. However, the regression was performed anyway.</h4>
```{R}
#install.packages("lmtest")
#install.packages("sandwich")
library(lmtest)
library(sandwich)

# mean center both numeric variables 
Quar_tidier$months_c <- Quar_tidier$months - mean(Quar_tidier$months)
Quar_tidier$numGroups_c <- Quar_tidier$numGroups - mean(Quar_tidier$numGroups)

# build a regression model predicting deaths from months, number of groups involved, and their interaction 
fit_num <- lm(deaths ~ months_c * numGroups_c, data = Quar_tidier)
summary(fit_num)

# regression interaction plot 
reg <-predict(fit_num, Quar_tidier)
ggplot(Quar_tidier, aes(numGroups_c, deaths)) + geom_point() + geom_line(data = Quar_tidier, aes(y = reg)) +
  ggtitle("Regression Plot")

# check assumptions
# check for homoskedasticity with the Breusch Pagan test 
resids <- fit_num$residuals; fitvals <- fit_num$fitted.values
# check for linearity by plotting the residuals 
ggplot() + geom_point(aes(fitvals, resids)) + geom_hline(yintercept=0, col="red") +
  ggtitle("Check for Homoskedasticity")
bptest(fit_num) 

# check for normality with the ks test and graphically by making a histogram of residuals 
ggplot()+geom_histogram(aes(resids),bins=20) + ggtitle("Check for Normality")
ks.test(resids, "pnorm", sd=sd(resids))

# recompute regression results with robust standard errors
coeftest(fit_num, vcov=vcovHC(fit_num))

```
*When regression results were rerun with robust standard errors, I found that numGroups was the only significant predictor of deaths (p-value of <2e-16). months and the interaction between numGroups and months were both non-significant predictors because they had p-values larger than 0.05. This means that controlling for months, there is a significant effect of numGroups on deaths. For every one unit increase in numGroups, the number of deaths increases by 9.339e+05 on average, t = 31.431, df = 427. With respect to significant predictors, there is no difference between before/after robust SEs. From the R-squared value, we can conclude that 71.19% of variation in the outcome is explained by this model.*




<h4>4. Reran same regression model (with interaction), but this time compute bootstrapped standard errors.</h4>
```{R}
# repeat 5000 times, saving the coefficients each time
samp_distn <- replicate(5000, {
 boot_dat <- Quar_tidier[sample(nrow(Quar_tidier),replace=TRUE),]
 fit_boot <- lm(deaths ~ months_c * numGroups_c, data = boot_dat)
 coef(fit_boot)
})

# estimated SEs
samp_distn %>% t %>% as.data.frame %>% summarize_all(sd)

```
*When computing bootstrapped errors, we see that the standard error for each of the variables and the interaction siginificantly increases compared to the original SEs. However, they decrease with respect to the robust standard errors. Therefore, the original SEs are probably the best ones to use for the purpose of building the model because we aim to decrease standard error. Becuase of the formula, if the standard error gets smaller, he t- statistic gets bigger and the p-value gets smaller. Therefore, from the previous conclusions about the change in standard error between the original, robust, and bootstrapped SEs, we can conclude that the original SE model would have the lowest p-values, followed by the bootstrapped SE model, and then the robust SE model.*
 
 
<h4>5. Next, I performed a logistic regression predicting the binary categorical variable 'sameLanguage' from all other variables in the datset. 'sameLanguage' is a binary variable which is 1 if the fighting groups share the same language and 0 if they hav different languages.</h4>
```{r}
#install.packages("plotROC")
library(plotROC) 

# predict the binary variable 'sameLanguage', from all the other variables in the dataset 
#fit_bin<-glm(sameLanguage ~ difRelig + difWealth + numGroups_c + difLegal + Scale_of_Conflict, data=Quar_tidier,family=binomial(link="logit"))
fit_bin<-glm(sameLanguage ~., data=Quar_tidier,family=binomial(link="logit"))
summary(fit_bin) # viewing the coefficients 
coef(fit_bin) %>% round(3) # original scale (log-odds scale)
exp(coef(fit_bin)) %>% round(3) # exponentiated (odds scale)

prob <- predict(fit_bin, type = "response")
truth <- Quar_tidier$sameLanguage

# confusion matrix
table(predict = as.numeric(prob > .5), truth) %>% addmargins

# calculate sensitivity (TPR), specificity (TNR), accuracy, and precision (PPV)
# accuracy
accu <- (330 + 39)/431
# sensitivity
sens <- 39/76
# specificity
spec <- 330/355
# precision
prec <- 39/64

# plot density of log-odds (logit) by your binary outcome variable
Quar_logit <- Quar_tidier
Quar_logit$logit<-predict(fit_bin) #get predicted log-odds
Quar_logit$language<-factor(Quar_logit$sameLanguage,levels=c("1","0"))
ggplot(Quar_logit,aes(logit, fill=language))+geom_density(alpha=.3)+
  geom_vline(xintercept=0,lty=2) + 
scale_fill_discrete(name = "Language", labels = c("same", "different"))

# ROC plot and AUC calculation
Quar_roc <- Quar_tidier
Quar_roc$sameLang <- ifelse(Quar_roc$sameLanguage == "1", 1, 0)
fit_bin2<-glm(sameLang ~., data=Quar_roc,family=binomial(link="logit"))
Quar_roc$prob <- predict(fit_bin2, data=Quar_roc, type = "response")
ROCplot <- ggplot(Quar_roc) + geom_roc(aes(d = sameLang, m = prob), n.cuts=0) + 
 geom_segment(aes(x=0,xend=1,y=0,yend=1),lty=2) + ggtitle("ROC Plot") 
ROCplot
calc_auc(ROCplot)

```
*Since the coefficient estimates were initially returned in the log form, I calculated the exponent of each of the coefficients before interpretation. After this, I found that the significant predictors of sameLanguage were difWealth (groups with a difference in wealth), difRelig (groups with different religions), and Scale_of_ConflictRevolution (the 'revolution' level of the categorical variable 'Scale_of_Conflict). The following are the interpretations of the coefficient estimates in terms of odds:*
*For difWealth, odds for having the same language for groups with wealth differences are 6.052 times that of groups with no wealth differences. For difRelig, odds for having the same language for groups with different religions are 0.2 times that of groups with the same religion. For Scale_of_Conflict, odds for having the same language for a revolution is 11.532 times that of colonial. The log(odds) equation is: -21.137 + 1.800difWealth1 - 1.607difRelig2 + 2.445Scale_of_Conflict:revolution.*
*The confusion matrix reports an accuracy of 0.856, a sensitivity of 0.513, a specificity of 0.930, and a precision of 0.609. The accuracy and specificity are relatively high, but the sensitivity and precision are low.*
*The ROC curve seems to match that of a perfectly predictive model, with AUC score of 1, which is classified as 'great'. However, that is unlikely to be the case here, so it is most likely due to some variational error.*




<h4>Perform 10-fold (or repeated random sub-sampling) CV and report average out-of-sample Accuracy, Sensitivity, and Recall</h4>
```{r}
# define the class_diag function
class_diag<-function(probs,truth){

tab<-table(factor(probs>.5,levels=c("FALSE","TRUE")),truth)
acc=sum(diag(tab))/sum(tab)
sens=tab[2,2]/colSums(tab)[2]
spec=tab[1,1]/colSums(tab)[1]
ppv=tab[2,2]/rowSums(tab)[2]

if(is.numeric(truth)==FALSE & is.logical(truth)==FALSE) truth<-as.numeric(truth)-1

# calculate AUC
ord<-order(probs, decreasing=TRUE)
probs <- probs[ord]; truth <- truth[ord]

TPR=cumsum(truth)/max(1,sum(truth)) 
FPR=cumsum(!truth)/max(1,sum(!truth))

dup<-c(probs[-1]>=probs[-length(probs)], FALSE)
TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)

n <- length(TPR)
auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )

data.frame(acc,sens,spec,ppv,auc)
}

set.seed(1234)
k=10

data1<-Quar_tidier[sample(nrow(Quar_tidier)),]
folds<-cut(seq(1:nrow(Quar_tidier)),breaks=k,labels=F)

diags<-NULL
for(i in 1:k){
train<-data1[folds!=i,]
test<-data1[folds==i,]
truth<-test$sameLanguage
fit_bin<-glm(sameLanguage ~ ., data=Quar_tidier,family=binomial(link="logit"))
probs<- predict(fit_bin, newdata=test, type="response")
diags<-rbind(diags,class_diag(probs,truth))}

apply(diags,2,mean)

```
*The AUC value from this 10-fold CV for the logistic regression model is 0.887, which is classified as 'good'. This means that the model is good at predicting the binary variable sameLanguage from all the other variables in the dataset. The accuracy, sensitivity, and precision was found to be 0.856, 0.471, and 0.546 respectively. The specificity was found to be 0.929.*
 
 
 
<h4>6. Lasso regression with the binary logistic regression from Part-5 followed by 10-fold CV</h4>
```{r}
#install.packages("glmnet")
library(glmnet)


# run the lasso regression to identify significant variables 
fit_lasso<- glm(sameLanguage~., data = Quar_tidier, family = binomial(link = "logit"))
x <- model.matrix(fit_lasso)[, -1] # predictor matrix 
y <- as.matrix(Quar_tidier$sameLanguage) # reponse variable 
cv1 <- cv.glmnet(x, y, family="binomial")
lasso <-glmnet(x, y, family="binomial", lambda=cv1$lambda.1se)
coef(lasso)

#add nonzero coefficient variables to a new dataset 
Quar_lasso <- Quar_tidier
Quar_lasso$Scale_of_ConflictRevolution <- ifelse(Quar_lasso$Scale_of_Conflict == "revolution", 1, 0)
Quar_lasso$difWealth1 <- ifelse(Quar_lasso$difWealth == "1", 1, 0)
Quar_lasso$difRelig2 <- ifelse(Quar_lasso$difRelig == "2", 1, 0)
Quar_lasso <- Quar_lasso %>% select(-Scale_of_Conflict, -difWealth, -difRelig) # remove the original columns 

# run the 10-fold CV
set.seed(1234)
k=10

Quar_data <- Quar_lasso[sample(nrow(Quar_lasso)),]
folds<-cut(seq(1:nrow(Quar_lasso)),breaks=k,labels=F)

diags<-NULL
for(i in 1:k){
train<-Quar_data[folds!=i,]
test<-Quar_data[folds==i,]
truth<-test$sameLanguage
fit_lasso2 <- glm(sameLanguage ~ difWealth1 + difRelig2 + Scale_of_ConflictRevolution, data = Quar_data,family=binomial(link="logit"))
probs<- predict(fit_lasso2, newdata=test, type="response")
diags<-rbind(diags,class_diag(probs,truth))}

apply(diags,2,mean)

```
*After conducting a lasso regression, I found that the variables retained included difWealth1 (conflict participants with differing levels of financial resource), difRelig2 (conflict participants with multiple different religions between them), and the revolution level of the categorical variable 'Scale_of_Conflict'. So, I created a new dataframe with only these variables of significance, and reconducted the 10-fold cross-validation model to find out what the effect on the AUC would be. The new AUC turned out to be 0.85, which is still classified as 'good' but is a little smaller than the AUC of 0.887 in the logistic regression from part 5. I am not sure why this is the case because we would expect the AUC to be higher when rerunning the regression after filtering out the non-significant variables from the model. Perhaps this has something to do with the fact that most of my dataset contained binary variables and not numerical ones.*
